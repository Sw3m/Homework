{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "#  Logistic Regression, Naive Bayes, and Clustering\n",
    "\n",
    "## Conceptual, Non-Programming\n",
    "\n",
    "### Naive Bayes\n",
    "* I have a bucket with 3 red balls and 7 blue balls in it. I reach into the bucket and grab a ball. What is the proability the ball is red, P(R)? \n",
    "#### 3/21 = 0.1429\n",
    "The probability that it's blue, P(B)?\n",
    "#### 7/21 = 0.3333\n",
    "* Now assume I have two buckets. Bucket 1, B1, have 5 red and 5 blue balls in it. Bucket 2, B2, has 2 red and 8 blue balls in it. \n",
    "    1. I first choose a bucket and then grab a ball. Assuming I choose bucket 1 and bucket 2 with equal probability, what is the probability that the chosen ball is red? blue?\n",
    "    #####P(R) = 7/20 = 0.35\n",
    "    #####P(B) = 5/20 = 0.25\n",
    "    \n",
    "    2. What is the conditional probability that I choose a red ball given B1, P(R|B1)?\n",
    "    #####P(R|B1) = 0.35/0.5 = 0.7\n",
    "    \n",
    "* Let's say we're performing a binary classification problem. We're given a binary class label y and a feature vector X with dimensionality m. Meaning there are m features.\n",
    "    1. What is Bayes theorem? Write it in terms of X and Y in the way it's used for our classification problem.\n",
    "    \n",
    "        $P(y|X1,...Xm)= \\frac{P(y) P(X1,...Xm|y)} {P(X1,...Xm)}$\n",
    "    2. In this problem we have 4 probabilities, {P(X), P(Y), P(X|Y), P(Y|X)}, which ones do we need to estimate?\n",
    "        ##### P(y)\n",
    "        \n",
    "* What is the Naive Bayes assumption? Why do we need to make this assumption?\n",
    "\n",
    "##### Naive Bayes assumes that the value of a particular feature is independant of the value of any other feature. This assumption needs to be made so that each distribution can be independently estimated as a one dimensional distribution that in turn helps to alleviate problems from dimnesionality.\n",
    "\n",
    "he decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality.\n",
    "* We discussed the following 3 different Naive Bayes implementations in SKLearn. When would you use each?\n",
    "    1. Gaussian?   - Use Gaussian for classification with known features.\n",
    "    2. Bernoulli?  - Use Bernoulli for classification for discrete data for boolean features.\n",
    "    3. Multinomial?  Use Multinomial for classification wtih discrete features like word counts for text classification.\n",
    " \n",
    "#### Difficult Naive Bayes\n",
    "Assume Y is boolean class label and X is a feature vector of dimension m so m is the number of features. Assume that each of the m attributes is a boolean. For example, each $x_i$ is 0 if word i isn't present and 1 otherwise. \n",
    "\n",
    "* Before the Naive assumption we need to estimate P(X|Y). \n",
    "    1. How many total parameters are there?\n",
    "    2. How many parameters do we need to estimate?\n",
    "* How many parameters after making the Naive assumptions?\n",
    "\n",
    "\n",
    "### Logistic Regression\n",
    "* What type of machine learning algorithm is logistic regression?\n",
    "    ######Supervised used for binary classification\n",
    "* In logistic regression we use the logistic function. What is the logistic function interpretted as?\n",
    "#####  The logistic function is interpretted as the input that is being modeled gets larger and positive it gets closer to 1. As it gets large and negative it gets closer to 0.\n",
    "* What is the assumption of logistic regression? What does the decision boundary look like?\n",
    "##### The dependant variables should be binary. Meaningful variables should be included, so that the model is fitted correctly. The decision boundary looks like a line.\n",
    "\n",
    "\n",
    "#### Difficult Logistic Regression\n",
    "* Can you show,  mathematically, the decision classifier of logistic regression?\n",
    "\n",
    "### Clustering\n",
    "* What type of algorithm is k-means clustering? Supervised or unsupervised?\n",
    "##### Unsupervised\n",
    "* Does k-means clustering have any parameters? If so what are they?\n",
    "##### Yes k-means requires the number of clusters\n",
    "* What are the assumptions of k-means clustering?\n",
    "##### Assumptions are that clusters are spherical, well seperated, are similar volumes, and have similar number of points.\n",
    "\n",
    "#### More Difficult\n",
    "##### Equivalence of Gaussian Naive Bayes and Logistic Regression\n",
    "Assume the following:\n",
    "* y is boolean (binary classification problem)\n",
    "* x is an m-dimensional feature vector where each feature is contiuous\n",
    "* P(X|Y) is normally distributted $P(x_{i}|y_{k})=\\frac{1}{{\\sigma_{i}\\sqrt{2\\pi}}}e^{\\frac{-(x-\\mu_{ik})^2}{2\\sigma_{i}^2}}$\n",
    "* Note $\\sigma_{i}$ does  not depend on y but does vary by feature\n",
    "* All x's are conditionally independant given y\n",
    "\n",
    "See if you can answer the following questions:\n",
    "\n",
    "* Using Bayes rule write P(Y|X). Note that this is the Gaussian Naive Bayes algorithm. The Gaussian Naive Bayes form of P(Y|X). Why?\n",
    "* Just a note: Remember that in Logistic Regression we can write $P(Y|X) = \\frac{exp(w _{0} + \\sum\\limits_{i=1}^m w_{i}X_{i})}{1+exp(w _{0} + \\sum\\limits_{i=1}^m w_{i}X_{i})}$\n",
    "* Show that the Gaussian Naive Bayes form of P(Y|X) is equivalent to P(Y|X) in Logistic Regression.\n",
    "* After showing the above equivalence, what are the values of $w_{0}$ and $w_{i}$?\n",
    "\n",
    "##### Logistic regression MLE (Maximum Likelihood Estimation)\n",
    "In the previous question we showed that under the naive assumption Logistic Regression and Gaussian Naive Bayes are equivalent. As a result, we could estimate the weights $w_{0}$ and $w_{i}$ from the GNB estimates. However, we'd like a different way to estimate the Logistic Regression P(Y|X). We'd like this because we want a way to estimate that is more general than in Naive Bayes and isn't restricted by its assumptions. We can do ths via the maximum likelihood estimator, MLE. Assuming the data is iid we can write the likelihood as $\\prod P(Y|X,W)$ so we want to find the weights, W, that maximize this value. Since the logarithm is a monotonically increasing function we can take the logorithm of the likelihood without changing the final answer. Taking the logarithm we get the log likelihood $l(W)= \\sum_{i=1}^{N} log(p(y_i|x_i, W))$. It turns out there is no closed form solution to this equation but we can used gradient based methods to find the maximum/minimum [gradient methods](https://en.wikipedia.org/wiki/Gradient_descent). \n",
    "* Find the gradient of the log-likelihood function\n",
    "* Read about the gradient descent/ascent algorithm\n",
    "* What is the update rule for gradient ascent in the case of the logistic regression log-likelihood?\n",
    "* Add a regularization term to the log-likelihood function so it becomes $\\sum_{i=1}^{N} log(p(y_i|x_i, W)) + \\frac{\\lambda}{2}||W||^2$. Note the similarity with the regularized ridge and lasso regression from before. The divide by 2 helps with cancelling later on when differentiating. \n",
    "* Recalculate the gradient of the new objective function. What is it?\n",
    "* What is the new update rule for the gradient based optimization \n",
    "* Try to implement this in Python for logistic regression so you can calculate the MLE estimates of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
